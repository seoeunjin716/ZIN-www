"""QLoRAë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ë° íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤."""

import os
from pathlib import Path
from typing import List, Optional, Tuple

import torch
from datasets import Dataset
from peft import LoraConfig, TaskType, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    GenerationConfig,
    Trainer,
    TrainingArguments,
)


class ChatService:
    """QLoRAë¥¼ ì‚¬ìš©í•œ ì±„íŒ… ë° íŒŒì¸íŠœë‹ ì„œë¹„ìŠ¤."""

    def __init__(
        self,
        model_path: Optional[str] = None,
        use_quantization: bool = True,
        quantization_bits: int = 4,
    ):
        """ChatService ì´ˆê¸°í™”.

        Args:
            model_path: ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.
            use_quantization: ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True).
            quantization_bits: ì–‘ìí™” ë¹„íŠ¸ ìˆ˜ (4 ë˜ëŠ” 8, ê¸°ë³¸ê°’: 4).
        """
        if model_path is None:
            # ê¸°ë³¸ ê²½ë¡œ: backend/app/models/midm
            current_dir = Path(__file__).parent
            model_path = str(current_dir.parent / "models" / "midm")

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

        self.model_path = model_path
        self.use_quantization = use_quantization
        self.quantization_bits = quantization_bits
        self.model: Optional[AutoModelForCausalLM] = None
        self.tokenizer: Optional[AutoTokenizer] = None
        self.peft_model = None

    def load_model(
        self,
        use_lora: bool = False,
        lora_r: int = 16,
        lora_alpha: int = 32,
        lora_dropout: float = 0.05,
        lora_target_modules: Optional[List[str]] = None,
    ) -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
        """QLoRA ëª¨ë¸ ë¡œë“œ.

        Args:
            use_lora: LoRA ì–´ëŒ‘í„° ì‚¬ìš© ì—¬ë¶€ (í•™ìŠµ ì‹œ True).
            lora_r: LoRA rank (ê¸°ë³¸ê°’: 16).
            lora_alpha: LoRA alpha (ê¸°ë³¸ê°’: 32).
            lora_dropout: LoRA dropout (ê¸°ë³¸ê°’: 0.05).
            lora_target_modules: LoRAë¥¼ ì ìš©í•  ëª¨ë“ˆ ëª©ë¡. Noneì´ë©´ ìë™ ì„ íƒ.

        Returns:
            (model, tokenizer) íŠœí”Œ.
        """
        print(f"ğŸ“¦ QLoRA ëª¨ë¸ ë¡œë”© ì¤‘: {self.model_path}")

        # ì–‘ìí™” ì„¤ì •
        quantization_config = None
        if self.use_quantization:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=(self.quantization_bits == 4),
                load_in_8bit=(self.quantization_bits == 8),
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )

        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.bfloat16 if not self.use_quantization else None,
        )

        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        # LoRA ì„¤ì • (í•™ìŠµ ì‹œ)
        if use_lora:
            if lora_target_modules is None:
                # Midm ëª¨ë¸ì˜ ê¸°ë³¸ íƒ€ê²Ÿ ëª¨ë“ˆ (Llama ê³„ì—´)
                lora_target_modules = [
                    "q_proj",
                    "k_proj",
                    "v_proj",
                    "o_proj",
                    "gate_proj",
                    "up_proj",
                    "down_proj",
                ]

            lora_config = LoraConfig(
                task_type=TaskType.CAUSAL_LM,
                r=lora_r,
                lora_alpha=lora_alpha,
                lora_dropout=lora_dropout,
                target_modules=lora_target_modules,
                bias="none",
            )

            self.peft_model = get_peft_model(self.model, lora_config)
            self.peft_model.print_trainable_parameters()
            print("âœ… LoRA ì–´ëŒ‘í„° ì¶”ê°€ ì™„ë£Œ")

        print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        return self.model, self.tokenizer

    def chat(
        self,
        message: str,
        history: Optional[List[dict]] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        do_sample: bool = True,
        **generation_kwargs,
    ) -> str:
        """ëŒ€í™” ìƒì„±.

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€.
            history: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ì„ íƒì ).
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜.
            temperature: ìƒì„± ì˜¨ë„.
            do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€.
            **generation_kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°.

        Returns:
            ìƒì„±ëœ ì‘ë‹µ í…ìŠ¤íŠ¸.

        Raises:
            ValueError: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°.
        """
        if self.model is None or self.tokenizer is None:
            raise ValueError(
                "ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ë©”ì‹œì§€ êµ¬ì„±
        messages = []

        # ì‹œìŠ¤í…œ ë©”ì‹œì§€
        messages.append(
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. í•œêµ­ì–´ë¡œ ëŒ€í™”í•©ë‹ˆë‹¤.",
            }
        )

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        if history:
            messages.extend(history)

        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": message})

        # í† í¬ë‚˜ì´ì €ë¡œ ë³€í™˜
        input_ids = self.tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,
            return_tensors="pt",
        ).to(self.model.device)

        # Generation Config ë¡œë“œ (ìˆëŠ” ê²½ìš°)
        generation_config = None
        generation_config_path = Path(self.model_path) / "generation_config.json"
        if generation_config_path.exists():
            generation_config = GenerationConfig.from_pretrained(self.model_path)

        # í…ìŠ¤íŠ¸ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=do_sample,
                generation_config=generation_config,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id,
                **generation_kwargs,
            )

        # ì‘ë‹µ ë””ì½”ë”©
        response = self.tokenizer.decode(
            outputs[0][input_ids.shape[1] :], skip_special_tokens=True
        )
        return response.strip()

    def train(
        self,
        dataset: Dataset,
        output_dir: str,
        num_train_epochs: int = 3,
        per_device_train_batch_size: int = 1,
        gradient_accumulation_steps: int = 4,
        learning_rate: float = 2e-4,
        warmup_steps: int = 100,
        logging_steps: int = 10,
        save_steps: int = 500,
        **training_kwargs,
    ) -> Trainer:
        """QLoRA íŒŒì¸íŠœë‹ ì‹¤í–‰.

        Args:
            dataset: í•™ìŠµ ë°ì´í„°ì…‹ (datasets.Dataset).
            output_dir: ëª¨ë¸ ì €ì¥ ê²½ë¡œ.
            num_train_epochs: í•™ìŠµ ì—í­ ìˆ˜.
            per_device_train_batch_size: ë””ë°”ì´ìŠ¤ë‹¹ ë°°ì¹˜ í¬ê¸°.
            gradient_accumulation_steps: ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜.
            learning_rate: í•™ìŠµë¥ .
            warmup_steps: ì›Œë°ì—… ìŠ¤í… ìˆ˜.
            logging_steps: ë¡œê¹… ìŠ¤í… ê°„ê²©.
            save_steps: ì €ì¥ ìŠ¤í… ê°„ê²©.
            **training_kwargs: ì¶”ê°€ í•™ìŠµ íŒŒë¼ë¯¸í„°.

        Returns:
            Trainer ì¸ìŠ¤í„´ìŠ¤.

        Raises:
            ValueError: ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ LoRAê°€ ì„¤ì •ë˜ì§€ ì•Šì€ ê²½ìš°.
        """
        if self.peft_model is None:
            raise ValueError(
                "LoRA ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model(use_lora=True)ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
        def preprocess_function(examples):
            # ë°ì´í„°ì…‹ í˜•ì‹ì— ë”°ë¼ ìˆ˜ì • í•„ìš”
            # ì˜ˆ: {"instruction": "...", "input": "...", "output": "..."}
            if "instruction" in examples:
                texts = []
                for i in range(len(examples["instruction"])):
                    instruction = examples["instruction"][i]
                    input_text = (
                        examples.get("input", [""])[i] if "input" in examples else ""
                    )
                    output = examples["output"][i]

                    if input_text:
                        text = f"### Instruction:\n{instruction}\n\n### Input:\n{input_text}\n\n### Response:\n{output}"
                    else:
                        text = f"### Instruction:\n{instruction}\n\n### Response:\n{output}"

                    texts.append(text)
            else:
                # ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìŒ í˜•ì‹
                texts = examples.get("text", examples.get("input", []))

            # í† í¬ë‚˜ì´ì§•
            model_inputs = self.tokenizer(
                texts,
                max_length=512,
                truncation=True,
                padding="max_length",
            )
            model_inputs["labels"] = model_inputs["input_ids"].copy()
            return model_inputs

        # ë°ì´í„° ì „ì²˜ë¦¬
        tokenized_dataset = dataset.map(
            preprocess_function,
            batched=True,
            remove_columns=dataset.column_names,
        )

        # í•™ìŠµ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=num_train_epochs,
            per_device_train_batch_size=per_device_train_batch_size,
            gradient_accumulation_steps=gradient_accumulation_steps,
            learning_rate=learning_rate,
            warmup_steps=warmup_steps,
            logging_steps=logging_steps,
            save_steps=save_steps,
            save_strategy="steps",
            evaluation_strategy="no",
            logging_dir=f"{output_dir}/logs",
            report_to="none",
            **training_kwargs,
        )

        # Trainer ìƒì„± ë° í•™ìŠµ
        trainer = Trainer(
            model=self.peft_model,
            args=training_args,
            train_dataset=tokenized_dataset,
            data_collator=lambda x: {
                "input_ids": torch.stack(
                    [torch.tensor(item["input_ids"]) for item in x]
                ),
                "attention_mask": torch.stack(
                    [torch.tensor(item["attention_mask"]) for item in x]
                ),
                "labels": torch.stack([torch.tensor(item["labels"]) for item in x]),
            },
        )

        print("ğŸš€ í•™ìŠµ ì‹œì‘...")
        trainer.train()
        print(f"âœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ì´ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)

        return trainer

    def save_lora_adapter(self, output_dir: str) -> None:
        """LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥.

        Args:
            output_dir: ì €ì¥ ê²½ë¡œ.

        Raises:
            ValueError: LoRA ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš°.
        """
        if self.peft_model is None:
            raise ValueError("LoRA ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

        self.peft_model.save_pretrained(output_dir)
        self.tokenizer.save_pretrained(output_dir)
        print(f"âœ… LoRA ì–´ëŒ‘í„°ê°€ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def load_lora_adapter(self, adapter_path: str) -> None:
        """LoRA ì–´ëŒ‘í„° ë¡œë“œ.

        Args:
            adapter_path: ì–´ëŒ‘í„° ê²½ë¡œ.

        Raises:
            ValueError: ê¸°ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì€ ê²½ìš°.
        """
        if self.model is None:
            raise ValueError(
                "ê¸°ë³¸ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_model()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”."
            )

        from peft import PeftModel

        self.peft_model = PeftModel.from_pretrained(self.model, adapter_path)
        print(f"âœ… LoRA ì–´ëŒ‘í„°ê°€ {adapter_path}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
