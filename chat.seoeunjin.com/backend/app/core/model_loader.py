"""ë¡œì»¬ HuggingFace ëª¨ë¸ ë¡œë”."""

import os
from pathlib import Path
from typing import TYPE_CHECKING, Optional, Tuple

if TYPE_CHECKING:
    from transformers import AutoModelForCausalLM, AutoTokenizer

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

    TRANSFORMERS_AVAILABLE = True
except ImportError:
    TRANSFORMERS_AVAILABLE = False
    # íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•œ ë”ë¯¸ íƒ€ì…
    AutoModelForCausalLM = None  # type: ignore
    AutoTokenizer = None  # type: ignore

# device_map ì‚¬ìš© ì‹œ accelerate í•„ìš”
try:
    import accelerate

    ACCELERATE_AVAILABLE = True
except ImportError:
    ACCELERATE_AVAILABLE = False


def load_midm_model(
    model_path: Optional[str] = None,
    torch_dtype: Optional[str] = None,
    device_map: str = "auto",
    trust_remote_code: bool = True,
) -> Tuple["AutoModelForCausalLM", "AutoTokenizer"]:
    """Midm ëª¨ë¸ì„ ë¡œì»¬ ê²½ë¡œì—ì„œ ë¡œë“œ.

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.
        torch_dtype: torch dtype (ê¸°ë³¸ê°’: None, "auto" ì‚¬ìš© ì‹œ None).
        device_map: ë””ë°”ì´ìŠ¤ ë§µí•‘ (ê¸°ë³¸ê°’: "auto").
        trust_remote_code: ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€ (ê¸°ë³¸ê°’: True).

    Returns:
        (model, tokenizer) íŠœí”Œ.
    """
    if model_path is None:
        # ê¸°ë³¸ ê²½ë¡œ: backend/app/models/midm
        current_dir = Path(__file__).parent
        model_path = str(current_dir.parent / "models" / "midm")

    # ê²½ë¡œ í™•ì¸
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")

    print(f"ğŸ“¦ ëª¨ë¸ ë¡œë”© ì¤‘: {model_path}")

    # accelerate í™•ì¸ (device_map ì‚¬ìš© ì‹œ í•„ìš”)
    if device_map and device_map != "cpu" and not ACCELERATE_AVAILABLE:
        raise ImportError(
            "device_mapì„ ì‚¬ìš©í•˜ë ¤ë©´ accelerateê°€ í•„ìš”í•©ë‹ˆë‹¤. "
            "`pip install accelerate`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    # ëª¨ë¸ ë¡œë“œ íŒŒë¼ë¯¸í„° ì¤€ë¹„
    load_kwargs = {
        "trust_remote_code": trust_remote_code,
    }

    # device_map ì„¤ì •
    if device_map:
        load_kwargs["device_map"] = device_map

    # dtype ì„¤ì • (torch_dtype ëŒ€ì‹  dtype ì‚¬ìš©)
    if torch_dtype and torch_dtype != "auto":
        import torch

        if torch_dtype == "float16":
            load_kwargs["dtype"] = torch.float16
        elif torch_dtype == "bfloat16":
            load_kwargs["dtype"] = torch.bfloat16
        elif torch_dtype == "float32":
            load_kwargs["dtype"] = torch.float32
        # "auto"ì¸ ê²½ìš° ë˜ëŠ” Noneì¸ ê²½ìš° dtype ì„¤ì •í•˜ì§€ ì•ŠìŒ

    # ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        **load_kwargs,
    )

    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_path)

    print("âœ… í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")

    return model, tokenizer


def create_midm_pipeline(
    model_path: Optional[str] = None,
    torch_dtype: Optional[str] = None,
    device_map: str = "auto",
    trust_remote_code: bool = True,
    **pipeline_kwargs,
):
    """Midm ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ HuggingFace pipeline ìƒì„±.

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.
        torch_dtype: torch dtype (ê¸°ë³¸ê°’: "auto").
        device_map: ë””ë°”ì´ìŠ¤ ë§µí•‘ (ê¸°ë³¸ê°’: "auto").
        trust_remote_code: ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€ (ê¸°ë³¸ê°’: True).
        **pipeline_kwargs: pipelineì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì.

    Returns:
        HuggingFace pipeline ê°ì²´.
    """
    model, tokenizer = load_midm_model(
        model_path=model_path,
        torch_dtype=torch_dtype,
        device_map=device_map,
        trust_remote_code=trust_remote_code,
    )

    # pipeline ìƒì„±
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        **pipeline_kwargs,
    )

    print("âœ… Pipeline ìƒì„± ì™„ë£Œ")

    return pipe


def load_midm_langchain_model(
    model_path: Optional[str] = None,
    torch_dtype: Optional[str] = None,
    device_map: str = "auto",
    trust_remote_code: bool = True,
    **pipeline_kwargs,
):
    """Midm ëª¨ë¸ì„ LangChain HuggingFacePipelineë¡œ ë¡œë“œ.

    Args:
        model_path: ëª¨ë¸ ê²½ë¡œ. Noneì´ë©´ ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©.
        torch_dtype: torch dtype (ê¸°ë³¸ê°’: "auto").
        device_map: ë””ë°”ì´ìŠ¤ ë§µí•‘ (ê¸°ë³¸ê°’: "auto").
        trust_remote_code: ì›ê²© ì½”ë“œ ì‹ ë¢° ì—¬ë¶€ (ê¸°ë³¸ê°’: True).
        **pipeline_kwargs: pipelineì— ì „ë‹¬í•  ì¶”ê°€ ì¸ì.

    Returns:
        HuggingFacePipeline ì¸ìŠ¤í„´ìŠ¤.
    """
    try:
        from langchain_huggingface import HuggingFacePipeline
    except ImportError:
        raise ImportError(
            "langchain-huggingfaceê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. "
            "`pip install langchain-huggingface`ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”."
        )

    # pipeline ìƒì„±
    pipe = create_midm_pipeline(
        model_path=model_path,
        torch_dtype=torch_dtype,
        device_map=device_map,
        trust_remote_code=trust_remote_code,
        **pipeline_kwargs,
    )

    # LangChain HuggingFacePipelineë¡œ ë˜í•‘
    llm = HuggingFacePipeline(pipeline=pipe)

    print("âœ… LangChain HuggingFacePipeline ìƒì„± ì™„ë£Œ")

    return llm
